{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Albert\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\Albert\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "from math import ceil\n",
    "from pattern.en import tokenize\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.models import load_model\n",
    "from keras.backend import clear_session\n",
    "\n",
    "np.random.seed(233)\n",
    "vocab_dim = 100\n",
    "maxlen = 200\n",
    "n_iterations = 20\n",
    "n_exposures = 10\n",
    "window_size = 7\n",
    "batch_size = 2048\n",
    "n_epochs = 10\n",
    "cpu_count = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    def __init__(self, file):\n",
    "        self.file = file\n",
    "        self.length = 0\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.length > 0:\n",
    "            return self.length\n",
    "        for line in open(self.file):\n",
    "            self.length += 1\n",
    "        return self.length\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for line in open(self.file):\n",
    "            data = json.loads(line)\n",
    "            yield data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_lines(file):\n",
    "    n = 0\n",
    "    for line in open(file):\n",
    "        n += 1\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_now():\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_train(file):\n",
    "    if os.path.exists('Word2Vec'):\n",
    "        return KeyedVectors.load('Word2Vec')\n",
    "    data = Loader(file)\n",
    "    model = Word2Vec(size = vocab_dim,\n",
    "                     min_count = n_exposures,\n",
    "                     window = window_size,\n",
    "                     workers = cpu_count,\n",
    "                     iter = n_iterations)\n",
    "    model.build_vocab(data)\n",
    "    model.train(data, total_examples = model.corpus_count, epochs = n_iterations)\n",
    "    wv = model.wv\n",
    "    del model\n",
    "    wv.save('Word2Vec')\n",
    "    return wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary(wv = None):\n",
    "    n = 0\n",
    "    w2indx = {}\n",
    "    embedding_weights = np.zeros((len(wv.vocab.keys()) + 1, vocab_dim))\n",
    "    for word in wv.vocab.keys():\n",
    "        n += 1\n",
    "        w2indx[word] = n\n",
    "        embedding_weights[n, :] = wv[word]\n",
    "    n += 1\n",
    "    return n, w2indx, embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(file_x, file_y, w2indx):\n",
    "    while True:\n",
    "        fx = open(file_x)\n",
    "        fy = open(file_y)\n",
    "        while True:\n",
    "            x = []\n",
    "            y = []\n",
    "            for i in range(batch_size):\n",
    "                txt = fy.readline()\n",
    "                if not txt:\n",
    "                    break\n",
    "                y.append(int(txt))\n",
    "                txt = json.loads(fx.readline())\n",
    "                x.append([w2indx.get(word, 0) for word in txt])\n",
    "            if x == []:\n",
    "                break\n",
    "            x = sequence.pad_sequences(x, maxlen = maxlen, padding='post', truncating='post')\n",
    "            yield (x, y)\n",
    "        fx.close()\n",
    "        fy.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(n_symbols, w2indx, embedding_weights, file_x, file_y):\n",
    "    print('[%s] Defining a Simple Keras Model...' % (time_now()))\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim = n_symbols,\n",
    "                        output_dim = vocab_dim,\n",
    "                        mask_zero = True,\n",
    "                        weights = [embedding_weights],\n",
    "                        input_length = maxlen))\n",
    "    model.add(LSTM(activation = 'sigmoid', units = 50, recurrent_activation = 'hard_sigmoid'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    print('[%s] Compiling the Model...' % (time_now()))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "    n_steps = ceil(num_lines(file_y) / batch_size)\n",
    "    \n",
    "    for i in range(n_epochs):\n",
    "        print(\"[%s] Training (epoch %d)...\" % (time_now(), i))\n",
    "        model.fit_generator(data_generator(file_x, file_y, w2indx), \n",
    "                            steps_per_epoch = n_steps, epochs = 1, verbose = 1)\n",
    "        model.save('lstm_epoch_%d.h5' % i)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    print('[%s] Training a Word2Vec model...' % (time_now()))\n",
    "    wv = word2vec_train('text.json')\n",
    "    n_symbols, w2indx, embedding_weights = create_dictionary(wv)\n",
    "    print('[%s] Setting up Arrays for Keras Embedding Layer...' % (time_now()))\n",
    "    model = train_lstm(n_symbols, w2indx, embedding_weights, 'data_x.json', 'data_y.json')\n",
    "    print('[%s] Finished.' % (time_now()))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-05-17 02:23:09] Training a Word2Vec model...\n",
      "[2018-05-17 02:23:12] Setting up Arrays for Keras Embedding Layer...\n",
      "[2018-05-17 02:23:12] Defining a Simple Keras Model...\n",
      "[2018-05-17 02:23:16] Compiling the Model...\n",
      "[2018-05-17 02:23:18] Training (epoch 0)...\n",
      "Epoch 1/1\n",
      "668/668 [==============================] - 396s 593ms/step - loss: 0.3509 - acc: 0.8448\n",
      "[2018-05-17 02:29:59] Training (epoch 1)...\n",
      "Epoch 1/1\n",
      "668/668 [==============================] - 395s 592ms/step - loss: 0.2066 - acc: 0.9228\n",
      "[2018-05-17 02:36:38] Training (epoch 2)...\n",
      "Epoch 1/1\n",
      "668/668 [==============================] - 395s 592ms/step - loss: 0.1877 - acc: 0.9309\n",
      "[2018-05-17 02:43:18] Training (epoch 3)...\n",
      "Epoch 1/1\n",
      "668/668 [==============================] - 395s 592ms/step - loss: 0.1771 - acc: 0.9355\n",
      "[2018-05-17 02:49:57] Training (epoch 4)...\n",
      "Epoch 1/1\n",
      "668/668 [==============================] - 395s 592ms/step - loss: 0.1695 - acc: 0.9389\n",
      "[2018-05-17 02:56:36] Training (epoch 5)...\n",
      "Epoch 1/1\n",
      "668/668 [==============================] - 395s 591ms/step - loss: 0.1633 - acc: 0.9416\n",
      "[2018-05-17 03:03:15] Training (epoch 6)...\n",
      "Epoch 1/1\n",
      "668/668 [==============================] - 395s 592ms/step - loss: 0.1583 - acc: 0.9438\n",
      "[2018-05-17 03:09:55] Training (epoch 7)...\n",
      "Epoch 1/1\n",
      "668/668 [==============================] - 395s 592ms/step - loss: 0.1545 - acc: 0.9455\n",
      "[2018-05-17 03:16:34] Training (epoch 8)...\n",
      "Epoch 1/1\n",
      "668/668 [==============================] - 395s 592ms/step - loss: 0.1481 - acc: 0.9484\n",
      "[2018-05-17 03:23:14] Training (epoch 9)...\n",
      "Epoch 1/1\n",
      "668/668 [==============================] - 395s 592ms/step - loss: 0.1427 - acc: 0.9508\n",
      "[2018-05-17 03:29:53] Finished.\n"
     ]
    }
   ],
   "source": [
    "model = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model 0] Precision = 0.917, Recall = 0.932, F1 = 0.925\n",
      "[[458.  42.]\n",
      " [ 34. 466.]\n",
      " [121. 129.]]\n",
      "\n",
      "[Model 1] Precision = 0.935, Recall = 0.916, F1 = 0.925\n",
      "[[468.  32.]\n",
      " [ 42. 458.]\n",
      " [130. 120.]]\n",
      "\n",
      "[Model 2] Precision = 0.935, Recall = 0.922, F1 = 0.928\n",
      "[[468.  32.]\n",
      " [ 39. 461.]\n",
      " [128. 122.]]\n",
      "\n",
      "[Model 3] Precision = 0.938, Recall = 0.934, F1 = 0.936\n",
      "[[469.  31.]\n",
      " [ 33. 467.]\n",
      " [122. 128.]]\n",
      "\n",
      "[Model 4] Precision = 0.943, Recall = 0.926, F1 = 0.934\n",
      "[[472.  28.]\n",
      " [ 37. 463.]\n",
      " [129. 121.]]\n",
      "\n",
      "[Model 5] Precision = 0.949, Recall = 0.924, F1 = 0.936\n",
      "[[475.  25.]\n",
      " [ 38. 462.]\n",
      " [129. 121.]]\n",
      "\n",
      "[Model 6] Precision = 0.952, Recall = 0.920, F1 = 0.936\n",
      "[[477.  23.]\n",
      " [ 40. 460.]\n",
      " [129. 121.]]\n",
      "\n",
      "[Model 7] Precision = 0.947, Recall = 0.926, F1 = 0.936\n",
      "[[474.  26.]\n",
      " [ 37. 463.]\n",
      " [130. 120.]]\n",
      "\n",
      "[Model 8] Precision = 0.946, Recall = 0.916, F1 = 0.931\n",
      "[[474.  26.]\n",
      " [ 42. 458.]\n",
      " [132. 118.]]\n",
      "\n",
      "[Model 9] Precision = 0.952, Recall = 0.920, F1 = 0.936\n",
      "[[477.  23.]\n",
      " [ 40. 460.]\n",
      " [133. 117.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wv = KeyedVectors.load('Word2Vec')\n",
    "_, w2indx, _ = create_dictionary(wv)\n",
    "with open('data_test_x.json') as f:\n",
    "    x = json.loads(f.read())\n",
    "with open('data_test_y.json') as f:\n",
    "    y = json.loads(f.read())\n",
    "for i in range(n_epochs):\n",
    "    model = load_model('lstm_epoch_%d.h5' % i)\n",
    "    n = np.zeros((3, 2))\n",
    "    xx = []\n",
    "    for j in range(len(y)):\n",
    "        xx.append([w2indx.get(word, 0) for word in x[j]])\n",
    "    xx = sequence.pad_sequences(xx, maxlen = maxlen, padding='post', truncating='post')\n",
    "    predict = model.predict_classes(xx)\n",
    "    del model\n",
    "    clear_session()\n",
    "    for j in range(len(y)):\n",
    "        n[y[j]][predict[j][0]] += 1\n",
    "    p = n[1][1] / (n[1][1] + n[0][1])\n",
    "    r = n[1][1] /(n[1][1] + n[1][0])\n",
    "    f1 = (2 * p * r) / (p + r)\n",
    "    print('[Model %d] Precision = %.3f, Recall = %.3f, F1 = %.3f' % (i, p, r, f1))\n",
    "    print(n)\n",
    "    print()\n",
    "del wv\n",
    "del w2indx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "x = [\"Good!\", \"Bad.\", \"I like this book!\", \"I like this book.\", \"I hate this book.\"]\n",
    "x = [' '.join(tokenize(re.sub('([a-z][.!?]+)([A-Z])', '\\g<1> \\g<2>', st, 0))).lower().split() for st in x]\n",
    "xx = []\n",
    "for st in x:\n",
    "    xx.append([w2indx.get(word, 0) for word in st])\n",
    "xx = sequence.pad_sequences(xx, maxlen = maxlen, padding='post', truncating='post')\n",
    "predict = model.predict_classes(xx)\n",
    "print(predict[:, 0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
